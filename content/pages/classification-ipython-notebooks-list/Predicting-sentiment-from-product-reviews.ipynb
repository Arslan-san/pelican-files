{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment from product reviews\n",
    "\n",
    "\n",
    "The goal of this first notebook is to explore logistic regression and feature engineering with existing GraphLab functions.\n",
    "\n",
    "In this notebook you will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n",
    "\n",
    "* Use SFrames to do some feature engineering\n",
    "* Train a logistic regression model to predict the sentiment of product reviews.\n",
    "* Inspect the weights (coefficients) of a trained logistic regression model.\n",
    "* Make a prediction (both class and probability) of sentiment for a new product review.\n",
    "* Given the logistic regression weights, predictors and ground truth labels, write a function to compute the **accuracy** of the model.\n",
    "* Inspect the coefficients of the logistic regression model and interpret their meanings.\n",
    "* Compare multiple logistic regression models.\n",
    "\n",
    "Let's get started!\n",
    "    \n",
    "## Fire up GraphLab Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of GraphLab Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import graphlab\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "We will use a dataset consisting of baby product reviews on Amazon.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to arslanahmadid@gmail.com and will expire on October 28, 2019.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.0.1 started. Logging: C:\\Users\\arslan\\AppData\\Local\\Temp\\graphlab_server_1551519673.log.0\n"
     ]
    }
   ],
   "source": [
    "products = graphlab.SFrame('amazon_baby.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see a preview of what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+--------+\n",
      "|              name             |             review            | rating |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "|    Planetwise Flannel Wipes   | These flannel wipes are OK... |  3.0   |\n",
      "|     Planetwise Wipe Pouch     | it came early and was not ... |  5.0   |\n",
      "| Annas Dream Full Quilt wit... | Very soft and comfortable ... |  5.0   |\n",
      "| Stop Pacifier Sucking with... | This is a product well wor... |  5.0   |\n",
      "| Stop Pacifier Sucking with... | All of my kids have cried ... |  5.0   |\n",
      "| Stop Pacifier Sucking with... | When the Binky Fairy came ... |  5.0   |\n",
      "| A Tale of Baby's Days with... | Lovely book, it's bound ti... |  4.0   |\n",
      "| Baby Tracker&reg; - Daily ... | Perfect for new parents. W... |  5.0   |\n",
      "| Baby Tracker&reg; - Daily ... | A friend of mine pinned th... |  5.0   |\n",
      "| Baby Tracker&reg; - Daily ... | This has been an easy way ... |  4.0   |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "[183531 rows x 3 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "print products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the word count vector for each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore a specific example of a baby product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'The First Years Massaging Action Teether',\n",
       " 'rating': 5.0,\n",
       " 'review': 'A favorite in our house!'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[269]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
    "2. Transform the reviews into word-counts.\n",
    "\n",
    "**Aside**. In this notebook, we remove all punctuations for the sake of simplicity. A smarter approach to punctuations would preserve phrases such as \"I'd\", \"would've\", \"hadn't\" and so forth. See [this page](https://www.cis.upenn.edu/~treebank/tokenization.html) for an example of smart handling of punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation) \n",
    "\n",
    "review_without_punctuation = products['review'].apply(remove_punctuation)\n",
    "products['word_count'] = graphlab.text_analytics.count_words(review_without_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These flannel wipes are OK, but in my opinion not worth keeping.  I also ordered someImse Vimse Cloth Wipes-Ocean Blue-12 countwhich are larger, had a nicer, softer texture and just seemed higher quality.  I use cloth wipes for hands and faces and have been usingThirsties 6 Pack Fab Wipes, Boyfor about 8 months now and need to replace them because they are starting to get rough and have had stink issues for a while that stripping no longer handles.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These flannel wipes are OK but in my opinion not worth keeping  I also ordered someImse Vimse Cloth WipesOcean Blue12 countwhich are larger had a nicer softer texture and just seemed higher quality  I use cloth wipes for hands and faces and have been usingThirsties 6 Pack Fab Wipes Boyfor about 8 months now and need to replace them because they are starting to get rough and have had stink issues for a while that stripping no longer handles'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_without_punctuation[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us explore what the sample example above looks like after these 2 transformations. Here, each entry in the **word_count** column is a dictionary where the key is the word and the value is a count of the number of times the word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1L, 'favorite': 1L, 'house': 1L, 'in': 1L, 'our': 1L}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[269]['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentiments\n",
    "\n",
    "We will **ignore** all reviews with *rating = 3*, since they tend to have a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166752"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "len(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assign reviews with a rating of 4 or higher to be *positive* reviews, while the ones with rating of 2 or lower are *negative*. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+--------+\n",
      "|              name             |             review            | rating |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "|     Planetwise Wipe Pouch     | it came early and was not ... |  5.0   |\n",
      "| Annas Dream Full Quilt wit... | Very soft and comfortable ... |  5.0   |\n",
      "| Stop Pacifier Sucking with... | This is a product well wor... |  5.0   |\n",
      "| Stop Pacifier Sucking with... | All of my kids have cried ... |  5.0   |\n",
      "| Stop Pacifier Sucking with... | When the Binky Fairy came ... |  5.0   |\n",
      "| A Tale of Baby's Days with... | Lovely book, it's bound ti... |  4.0   |\n",
      "| Baby Tracker&reg; - Daily ... | Perfect for new parents. W... |  5.0   |\n",
      "| Baby Tracker&reg; - Daily ... | A friend of mine pinned th... |  5.0   |\n",
      "| Baby Tracker&reg; - Daily ... | This has been an easy way ... |  4.0   |\n",
      "| Baby Tracker&reg; - Daily ... | I love this journal and ou... |  4.0   |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "+-------------------------------+-----------+\n",
      "|           word_count          | sentiment |\n",
      "+-------------------------------+-----------+\n",
      "| {'and': 3L, 'love': 1L, 'i... |     1     |\n",
      "| {'and': 2L, 'quilt': 1L, '... |     1     |\n",
      "| {'and': 3L, 'ingenious': 1... |     1     |\n",
      "| {'and': 2L, 'all': 2L, 'he... |     1     |\n",
      "| {'and': 2L, 'cute': 1L, 'h... |     1     |\n",
      "| {'shop': 1L, 'be': 1L, 'is... |     1     |\n",
      "| {'and': 2L, 'all': 1L, 'ri... |     1     |\n",
      "| {'and': 1L, 'fantastic': 1... |     1     |\n",
      "| {'all': 1L, 'standarad': 1... |     1     |\n",
      "| {'all': 2L, 'nannys': 1L, ... |     1     |\n",
      "+-------------------------------+-----------+\n",
      "[166752 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)\n",
    "print products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that the dataset contains an extra column called **sentiment** which is either positive (+1) or negative (-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133416\n",
      "33336\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = products.random_split(.8, seed=1)\n",
    "print len(train_data)\n",
    "print len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a sentiment classifier with logistic regression\n",
    "\n",
    "We will now use logistic regression to create a sentiment classifier on the training data. This model will use the column **word_count** as a feature and the column **sentiment** as the target. We will use `validation_set=None` to obtain same results as everyone else.\n",
    "\n",
    "**Note:** This line may take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Logistic regression:</pre>"
      ],
      "text/plain": [
       "Logistic regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 133416</pre>"
      ],
      "text/plain": [
       "Number of examples          : 133416"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 1</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 121712</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 121712"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 121713</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 121713"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting L-BFGS</pre>"
      ],
      "text/plain": [
       "Starting L-BFGS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 5        | 0.000002  | 2.760496     | 0.840754          |</pre>"
      ],
      "text/plain": [
       "| 1         | 5        | 0.000002  | 2.760496     | 0.840754          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 9        | 3.000000  | 4.320156     | 0.931350          |</pre>"
      ],
      "text/plain": [
       "| 2         | 9        | 3.000000  | 4.320156     | 0.931350          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 10       | 3.000000  | 4.951796     | 0.882046          |</pre>"
      ],
      "text/plain": [
       "| 3         | 10       | 3.000000  | 4.951796     | 0.882046          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 11       | 3.000000  | 5.757350     | 0.954076          |</pre>"
      ],
      "text/plain": [
       "| 4         | 11       | 3.000000  | 5.757350     | 0.954076          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 5         | 12       | 3.000000  | 6.415571     | 0.960964          |</pre>"
      ],
      "text/plain": [
       "| 5         | 12       | 3.000000  | 6.415571     | 0.960964          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 6         | 13       | 3.000000  | 6.993271     | 0.975033          |</pre>"
      ],
      "text/plain": [
       "| 6         | 13       | 3.000000  | 6.993271     | 0.975033          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>TERMINATED: Terminated due to numerical difficulties.</pre>"
      ],
      "text/plain": [
       "TERMINATED: Terminated due to numerical difficulties."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>This model may not be ideal. To improve it, consider doing one of the following:\n",
       "(a) Increasing the regularization.\n",
       "(b) Standardizing the input data.\n",
       "(c) Removing highly correlated features.\n",
       "(d) Removing `inf` and `NaN` values in the training data.</pre>"
      ],
      "text/plain": [
       "This model may not be ideal. To improve it, consider doing one of the following:\n",
       "(a) Increasing the regularization.\n",
       "(b) Standardizing the input data.\n",
       "(c) Removing highly correlated features.\n",
       "(d) Removing `inf` and `NaN` values in the training data."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_model = graphlab.logistic_classifier.create(train_data,\n",
    "                                                      target = 'sentiment',\n",
    "                                                      features=['word_count'],\n",
    "                                                      validation_set=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class                          : LogisticClassifier\n",
       "\n",
       "Schema\n",
       "------\n",
       "Number of coefficients         : 121713\n",
       "Number of examples             : 133416\n",
       "Number of classes              : 2\n",
       "Number of feature columns      : 1\n",
       "Number of unpacked features    : 121712\n",
       "\n",
       "Hyperparameters\n",
       "---------------\n",
       "L1 penalty                     : 0.0\n",
       "L2 penalty                     : 0.01\n",
       "\n",
       "Training Summary\n",
       "----------------\n",
       "Solver                         : lbfgs\n",
       "Solver iterations              : 6\n",
       "Solver status                  : TERMINATED: Terminated due to numerical difficulties.\n",
       "Training time (sec)            : 7.553\n",
       "\n",
       "Settings\n",
       "--------\n",
       "Log-likelihood                 : inf\n",
       "\n",
       "Highest Positive Coefficients\n",
       "-----------------------------\n",
       "word_count[mobileupdate]       : 41.9847\n",
       "word_count[placeid]            : 41.7354\n",
       "word_count[labelbox]           : 41.151\n",
       "word_count[httpwwwamazoncomreviewrhgg6qp7tdnhbrefcmcrprcmtieutf8asinb00318cla0nodeid] : 40.0454\n",
       "word_count[knobskeeping]       : 36.2091\n",
       "\n",
       "Lowest Negative Coefficients\n",
       "----------------------------\n",
       "word_count[probelm]            : -44.9283\n",
       "word_count[impulsejeep]        : -43.081\n",
       "word_count[infantsyoung]       : -39.5945\n",
       "word_count[cutereditafter]     : -35.6875\n",
       "word_count[avacado]            : -35.0542"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**. You may get a warning to the effect of \"Terminated due to numerical difficulties --- this model may not be ideal\". It means that the quality metric (to be covered in Module 3) failed to improve in the last iteration of the run. The difficulty arises as the sentiment model puts too much weight on extremely rare words. A way to rectify this is to apply regularization, to be covered in Module 4. Regularization lessens the effect of extremely rare words. For the purpose of this assignment, however, please proceed with the model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted the model, we can extract the weights (coefficients) as an SFrame as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+-------+------------------+--------+\n",
      "|     name    |    index     | class |      value       | stderr |\n",
      "+-------------+--------------+-------+------------------+--------+\n",
      "| (intercept) |     None     |   1   |  1.30337080544   |  None  |\n",
      "|  word_count |  recommend   |   1   |  0.303815600015  |  None  |\n",
      "|  word_count |    highly    |   1   |  1.49183015276   |  None  |\n",
      "|  word_count | disappointed |   1   |  -3.95748618393  |  None  |\n",
      "|  word_count |     love     |   1   |  1.43301685439   |  None  |\n",
      "|  word_count |      it      |   1   | 0.00986646490307 |  None  |\n",
      "|  word_count |    planet    |   1   | -0.797764553926  |  None  |\n",
      "|  word_count |     and      |   1   |  0.048449573172  |  None  |\n",
      "|  word_count |     bags     |   1   |  0.165541436615  |  None  |\n",
      "|  word_count |    wipes     |   1   | -0.0949937947268 |  None  |\n",
      "+-------------+--------------+-------+------------------+--------+\n",
      "[121713 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
      "['name', 'index', 'class', 'value', 'stderr']\n"
     ]
    }
   ],
   "source": [
    "weights = sentiment_model.coefficients\n",
    "print weights\n",
    "print weights.column_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of `121713` coefficients in the model. Recall from the lecture that positive weights $w_j$ correspond to weights that cause positive sentiment, while negative weights correspond to negative sentiment. \n",
    "\n",
    "Fill in the following block of code to calculate how many *weights* are positive ( >= 0). (**Hint**: The `'value'` column in SFrame *weights* must be positive ( >= 0))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121713\n",
      "Number of positive weights: 68419 \n",
      "Number of negative weights: 53294 \n",
      "121713\n"
     ]
    }
   ],
   "source": [
    "print len(weights)\n",
    "sperate_positive_weights = weights[weights['value'] >= 0]\n",
    "sperate_negative_weights = weights[weights['value'] < 0]\n",
    "num_positive_weights = len(sperate_positive_weights)\n",
    "num_negative_weights = len(sperate_negative_weights)\n",
    "\n",
    "print \"Number of positive weights: %s \" % num_positive_weights\n",
    "print \"Number of negative weights: %s \" % num_negative_weights\n",
    "print num_positive_weights + num_negative_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How many weights are >= 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with logistic regression\n",
    "\n",
    "Now that a model is trained, we can make predictions on the **test data**. In this section, we will explore this in the context of 3 examples in the test dataset.  We refer to this set of 3 examples as the **sample_test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, 2.0, 1.0]\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "|              name             |             review            | rating |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "|   Our Baby Girl Memory Book   | Absolutely love it and all... |  5.0   |\n",
      "| Wall Decor Removable Decal... | Would not purchase again o... |  2.0   |\n",
      "| New Style Trailing Cherry ... | Was so excited to get this... |  1.0   |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "+-------------------------------+-----------+\n",
      "|           word_count          | sentiment |\n",
      "+-------------------------------+-----------+\n",
      "| {'and': 2L, 'all': 1L, 'lo... |     1     |\n",
      "| {'and': 1L, 'wall': 1L, 't... |     -1    |\n",
      "| {'all': 1L, 'money': 1L, '... |     -1    |\n",
      "+-------------------------------+-----------+\n",
      "[3 rows x 5 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_test_data = test_data[10:13]\n",
    "print sample_test_data['rating']\n",
    "print sample_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig deeper into the first row of the **sample_test_data**. Here's the full review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Absolutely love it and all of the Scripture in it.  I purchased the Baby Boy version for my grandson when he was born and my daughter-in-law was thrilled to receive the same book again.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data[0]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That review seems pretty positive.\n",
    "\n",
    "Now, let's see what the next row of the **sample_test_data** looks like. As we could guess from the sentiment (-1), the review is quite negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Would not purchase again or recommend. The decals were thick almost plastic like and were coming off the wall as I was applying them! The would NOT stick! Literally stayed stuck for about 5 minutes then started peeling off.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data[1]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make a **class** prediction for the **sample_test_data**. The `sentiment_model` should predict **+1** if the sentiment is positive and **-1** if the sentiment is negative. Recall from the lecture that the **score** (sometimes called **margin**) for the logistic regression model  is defined as:\n",
    "\n",
    "$$\n",
    "\\mbox{score}_i = \\mathbf{w}^T h(\\mathbf{x}_i)\n",
    "$$ \n",
    "\n",
    "where $h(\\mathbf{x}_i)$ represents the features for example $i$.  We will write some code to obtain the **scores** using GraphLab Create. For each row, the **score** (or margin) is a number in the range **[-inf, inf]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.734619727058284, -5.734130996759765, -14.668460404467742]\n"
     ]
    }
   ],
   "source": [
    "scores = sentiment_model.predict(sample_test_data, output_type='margin')\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting sentiment\n",
    "\n",
    "These scores can be used to make class predictions as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{w}^T h(\\mathbf{x}_i) > 0 \\\\\n",
    "      -1 & \\mathbf{w}^T h(\\mathbf{x}_i) \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Using scores, write code to calculate $\\hat{y}$, the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "for i in scores:\n",
    "    if i >=0:\n",
    "        print \"1\"\n",
    "    else:\n",
    "        print \"-1\"\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from GraphLab Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions according to GraphLab Create:\n",
      "[1L, -1L, -1L]\n"
     ]
    }
   ],
   "source": [
    "print \"Class predictions according to GraphLab Create:\" \n",
    "print sentiment_model.predict(sample_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: Make sure your class predictions match with the one obtained from GraphLab Create.\n",
    "\n",
    "### Probability predictions\n",
    "\n",
    "Recall from the lectures that we can also calculate the probability predictions from the scores using:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))}.\n",
    "$$\n",
    "\n",
    "Using the variable **scores** calculated previously, write code to calculate the probability that a sentiment is positive using the above formula. For each row, the probabilities should be a number in the range **[0, 1]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998812384838\n",
      "0.0032232681818\n",
      "4.26155799666e-07\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for x in scores:\n",
    "    exp = math.exp( -x )\n",
    "    prob = 1/ (1+exp)\n",
    "    print prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: Make sure your probability predictions match the ones obtained from GraphLab Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions according to GraphLab Create:\n",
      "[0.9988123848377186, 0.003223268181802263, 4.261557996658726e-07]\n"
     ]
    }
   ],
   "source": [
    "print \"Class predictions according to GraphLab Create:\" \n",
    "print sentiment_model.predict(sample_test_data, output_type='probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Of the three data points in **sample_test_data**, which one (first, second, or third) has the **lowest probability** of being classified as a positive review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the most positive (and negative) review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to examining the full test dataset, **test_data**, and use GraphLab Create to form predictions on all of the test data points for faster performance.\n",
    "\n",
    "Using the `sentiment_model`, find the 20 reviews in the entire **test_data** with the **highest probability** of being classified as a **positive review**. We refer to these as the \"most positive reviews.\"\n",
    "\n",
    "To calculate these top-20 reviews, use the following steps:\n",
    "1.  Make probability predictions on **test_data** using the `sentiment_model`. (**Hint:** When you call `.predict` to make predictions on the test data, use option `output_type='probability'` to output the probability rather than just the most likely class.)\n",
    "2.  Sort the data according to those predictions and pick the top 20. (**Hint:** You can use the `.topk` method on an SFrame to find the top k rows sorted according to the value of a specified column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+--------+\n",
      "|              name             |             review            | rating |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "| Baby Tracker&reg; - Daily ... | This has been an easy way ... |  4.0   |\n",
      "| Baby Tracker&reg; - Daily ... | I love this journal and ou... |  4.0   |\n",
      "| Nature's Lullabies First Y... | I love this little calende... |  5.0   |\n",
      "| Nature's Lullabies Second ... | I had a hard time finding ... |  5.0   |\n",
      "|  Lamaze Peekaboo, I Love You  | One of baby's first and fa... |  4.0   |\n",
      "|  Lamaze Peekaboo, I Love You  | My son loved this book as ... |  5.0   |\n",
      "|  Lamaze Peekaboo, I Love You  | Our baby loves this book &... |  5.0   |\n",
      "| SoftPlay Giggle Jiggle Fun... | This bear is absolutely ad... |  2.0   |\n",
      "| SoftPlay Peek-A-Boo Where'... | I bought two for recent ba... |  5.0   |\n",
      "| Baby's First Year Undated ... | I searched high and low fo... |  5.0   |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "+-------------------------------+-----------+\n",
      "|           word_count          | sentiment |\n",
      "+-------------------------------+-----------+\n",
      "| {'all': 1L, 'standarad': 1... |     1     |\n",
      "| {'all': 2L, 'nannys': 1L, ... |     1     |\n",
      "| {'and': 1L, 'babys': 1L, '... |     1     |\n",
      "| {'and': 3L, 'all': 1L, 'mo... |     1     |\n",
      "| {'and': 2L, 'because': 1L,... |     1     |\n",
      "| {'all': 1L, 'being': 1L, '... |     1     |\n",
      "| {'and': 1L, 'own': 1L, 'it... |     1     |\n",
      "| {'and': 3L, 'cute': 1L, 'r... |     -1    |\n",
      "| {'beautiful': 1L, 'and': 2... |     1     |\n",
      "| {'remembering': 1L, 'and':... |     1     |\n",
      "+-------------------------------+-----------+\n",
      "[33336 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "print test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+--------+\n",
      "|              name             |             review            | rating |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "| Baby Tracker&reg; - Daily ... | This has been an easy way ... |  4.0   |\n",
      "| Baby Tracker&reg; - Daily ... | I love this journal and ou... |  4.0   |\n",
      "| Nature's Lullabies First Y... | I love this little calende... |  5.0   |\n",
      "| Nature's Lullabies Second ... | I had a hard time finding ... |  5.0   |\n",
      "|  Lamaze Peekaboo, I Love You  | One of baby's first and fa... |  4.0   |\n",
      "|  Lamaze Peekaboo, I Love You  | My son loved this book as ... |  5.0   |\n",
      "|  Lamaze Peekaboo, I Love You  | Our baby loves this book &... |  5.0   |\n",
      "| SoftPlay Giggle Jiggle Fun... | This bear is absolutely ad... |  2.0   |\n",
      "| SoftPlay Peek-A-Boo Where'... | I bought two for recent ba... |  5.0   |\n",
      "| Baby's First Year Undated ... | I searched high and low fo... |  5.0   |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "+-------------------------------+-----------+----------------+\n",
      "|           word_count          | sentiment |  probability   |\n",
      "+-------------------------------+-----------+----------------+\n",
      "| {'all': 1L, 'standarad': 1... |     1     | 0.758399887752 |\n",
      "| {'all': 2L, 'nannys': 1L, ... |     1     | 0.999999999966 |\n",
      "| {'and': 1L, 'babys': 1L, '... |     1     | 0.22895097808  |\n",
      "| {'and': 3L, 'all': 1L, 'mo... |     1     | 0.999999558063 |\n",
      "| {'and': 2L, 'because': 1L,... |     1     | 0.990542169248 |\n",
      "| {'all': 1L, 'being': 1L, '... |     1     | 0.999999295968 |\n",
      "| {'and': 1L, 'own': 1L, 'it... |     1     | 0.99976447628  |\n",
      "| {'and': 3L, 'cute': 1L, 'r... |     -1    | 0.722834466283 |\n",
      "| {'beautiful': 1L, 'and': 2... |     1     | 0.999266840896 |\n",
      "| {'remembering': 1L, 'and':... |     1     | 0.999786830048 |\n",
      "+-------------------------------+-----------+----------------+\n",
      "[33336 rows x 6 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "test_data['probability'] = sentiment_model.predict(test_data, output_type='probability')\n",
    "print test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 rows that has positive reviews based on probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britax Decathlon Convertible Car Seat, Tiffany == 1 == 1.0\n",
      "Ameda Purely Yours Breast Pump - Carry All == 1 == 1.0\n",
      "Traveling Toddler Car Seat Travel Accessory == -1 == 1.0\n",
      "Shermag Glider Rocker Combo, Pecan with Oatmeal == 1 == 1.0\n",
      "Cloud b Sound Machine Soother, Sleep Sheep == 1 == 1.0\n",
      "JP Lizzy Chocolate Ice Classic Tote Set == 1 == 1.0\n",
      "Fisher-Price Rainforest Melodies and Lights Deluxe Gym == 1 == 1.0\n",
      "Lilly Gold Sit 'n' Stroll 5 in 1 Car Seat and Stroller Combination, Tuxedo Black (sunshade is not included in the offering) == 1 == 1.0\n",
      "Fisher-Price Deluxe Jumperoo == 1 == 1.0\n",
      "North States Supergate Pressure Mount Clear Choice Wood Gate == 1 == 1.0\n",
      "Munchkin Mozart Magic Cube == 1 == 1.0\n",
      "Britax Marathon Convertible Car Seat, Granite == 1 == 1.0\n",
      "Wizard Convertible Car Seat with LATCH in Midnight Print == 1 == 1.0\n",
      "Capri Stroller - Red Tech == 1 == 1.0\n",
      "Peg Perego Primo Viaggio Car Seat / Infant Carrier with LATCH Base - Black Sable == 1 == 1.0\n",
      "HALO SleepSack Micro-Fleece Wearable Blanket, Soft Pink, Small == 1 == 1.0\n",
      "Leachco Snoogle Total Body Pillow == 1 == 1.0\n",
      "Summer Infant Complete Nursery Care Kit == 1 == 1.0\n",
      "Safety 1st Tot-Lok Four Lock Assembly == 1 == 1.0\n",
      "BABYBJORN Potty Chair - Red == 1 == 1.0\n"
     ]
    }
   ],
   "source": [
    "#print test_data[-1]\n",
    "twenty_rows = test_data.topk('probability',k=20)\n",
    "\n",
    "len(twenty_rows)\n",
    "for i in twenty_rows:\n",
    "    print i['name'],'==',i['sentiment'],'==',i[\"probability\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I researched a few different seats to put in our new Odyssey van in September (07). Our daughter just turned two and we wanted to get a new car seat for her so her Evenflo Triumph could go in my husbands car. She had been using the Triumph since she was about 9 months (rear-facing), and then forward facing from about 14 months. We really loved the Triumph, but I wanted something she could grow into. Meaning - getting the most out of a car seat height and weight wise for the longest period of time.Britax was one of the only ones I found, that actually makes a heavy-duty quality seat that I knew would last. Sure, I looked at a few others that ranked up there with height and weight (i.e. Eddie Bauer, Cosco, and Graco), but none of these compared in quality and durability! We made the trip to Babies r Us so we could \"test\" the few different seats I had chosen from Consumer Reports ratings. The Eddie Bauer deluxe booster was out because the first time I snapped the metal part into the crotch strap I pinched my daughters skin and she screamed like crazy! I felt so bad, and my husband immediately said, \"That's it for this one!\" and put it back on the shelf.The Eddie Bauer would have been the only other one we would have considered because it was more durable and sturdy than the Cosco and Graco. We did try the Britax Boulevard, but found it rather cumbersome to adjust the head rest and my daughter's head seemed to lean too far forward with the pillow being there. Overall, she looked uncomfortable in the Boulevard. We also tried the Britax Regent. Talk about a monstrosity of a car seat, but we both loved it, especially my hubby! It held my daughter in snug and secure and was easy to install into the test seat they have at the Babies r Us store. My taller than average daughter looked dinky in the Regent! The only reason we decided not to get the Regent was because our daughter still sleeps in her car seat on longer trips and the Regent just wasn't going to give her the head support and comfort level I wanted her to have while snoozing. By not buying the Regent as opposed to the Decathlon, we only gave up a 15 pound weight and 4\" height ability and we knew our daughter was going to outgrow the height limit WAY before she outgrew the weight limit. When that happens we'll probably just go to a standard backless booster that she'll use with the seatbelt from the car.Well, BrU didn't have the Decathlon at the store, and it wasn't one I was considering. I had researched the Marathon on Amazon and liked the reviews, but after comparing numerous reviews of the two seats. I decided on the Decathlon for three reasons: 1) It had the infant padding for rear-facing which will be good for our next child that comes along; 2) It had the toddler pillow; and 3) It had the adjustable crotch strap which I knew I would need for my daughter (she's in the 97th percentile for height, being 36\" tall at 24 months!). Essentially, the Decathlon and Marathon have the same seat structure and height and weight capacities, but the Decathlon has the \"extras\" I mentioned. Also, the Decathlon has a push-button strap release and tightener. The Marathon has a clasp crimper to hold the straps tight. Knowing the seat structures were basically the same, I toted my daughter over to Target and tried her in the Marathon, and I liked it, so I bought the Decathlon on Ebay.It is so easy to install once you've looked over the directions. Of course, at first it took some time, but now it's a breeze. The thing is rock solid in the captain's chair of our Odyssey. With the anchor belt and the tether anchor attached this baby isn't going anywhere unless the whole captain's chair comes off! Even without the tether anchor it still didn't move. The material is nice and heavy and the seat is adequately padded. It sits up high so my daughter can see everything outside. I love the \"hugs\" system the straps have because it makes my daughter fit snuggly into the seat.I only have two complaints about this seat, which really aren't that bad, just annoyances - 1) Sometimes the straps do twist a bit and I have to untwist them, especially with an independent toddler who wants to push their own arms through the straps and 2) It does take a little bit of strength to tighten down the straps with the push-button strap holder. I usually pull it as tight as I can and then if it still has too much slack I push the button with my left thumb and pull the strap with my right hand until it's tightened to my satisfaction.Overall, I've been very happy with the Britax Decathlon car seat. It was a good investment and as many of the reveiwers have stated - the price is worth every penny knowing that your child is protected very well. One bit of advise I'd give to anyone considering ANY car seat - GO TO A STORE WHERE YOU CAN TEST THE SEATS OUT ON YOUR CHILD! All children are different shapes and sizes and the seats are different. We are so glad we took our daughter to BrU and put her in each seat we were considering - and look, we didn't even buy any of the seats we were originally considering based on the Consumer Reports ratings I looked at! You just never know!Update - November 11, 2008Still loving this carseat. I had to take everything apart to clean it a couple weeks ago. It was a time consuming task, but not hard. The seat cover cleaned up beautifully and it looks brand new! Unfortunately, my daughter is now three and just about to outgrow the top slots (she's 41\" now!). She probably has another couple months in it. I'd really like to keep her in a harness for a lot longer, so I'm now checking out the new Graco Nautilus 3-in-1 carseat that's on the market. It's getting awesome reviews and it's a fraction of the cost of this seat or any of the other Britax's. Baby #2 will be here in December, so this one will be put away for when he/she is out of the infant carrier and this seat is big enough to keep the new little one rear-facing for a longer time!\n"
     ]
    }
   ],
   "source": [
    "print twenty_rows['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which of the following products are represented in the 20 most positive reviews? [multiple choice]\n",
    "\n",
    "\n",
    "Now, let us repeat this exercise to find the \"most negative reviews.\" Use the prediction probabilities to find the  20 reviews in the **test_data** with the **lowest probability** of being classified as a **positive review**. Repeat the same steps above but make sure you **sort in the opposite order**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Top 20 rows that has negative reviews based on probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jolly Jumper Arctic Sneak A Peek Infant Car Seat Cover Black == 1 == 7.80415068215e-100\n",
      "Levana Safe N'See Digital Video Baby Monitor with Talk-to-Baby Intercom and Lullaby Control (LV-TW501) == -1 == 6.83650885517e-25\n",
      "Snuza Portable Baby Movement Monitor == -1 == 2.12654510825e-24\n",
      "Fisher-Price Ocean Wonders Aquarium Bouncer == -1 == 2.2458208078e-23\n",
      "VTech Communications Safe &amp; Sounds Full Color Video and Audio Monitor == -1 == 1.32962966147e-22\n",
      "Safety 1st High-Def Digital Monitor == -1 == 2.0687209747e-20\n",
      "Chicco Cortina KeyFit 30 Travel System in Adventure == -1 == 5.93881994674e-20\n",
      "Prince Lionheart Warmies Wipes Warmer == -1 == 6.28510016543e-20\n",
      "Valco Baby Tri-mode Twin Stroller EX- Hot Chocolate == -1 == 8.05528712696e-20\n",
      "Adiri BPA Free Natural Nurser Ultimate Bottle Stage 1 White, Slow Flow (0-3 months) == -1 == 8.4652172495e-20\n",
      "Munchkin Nursery Projector and Sound System, White == -1 == 1.52853945171e-19\n",
      "The First Years True Choice P400 Premium Digital Monitor, 2 Parent Unit == -1 == 1.77901889374e-19\n",
      "Nuby Natural Touch Silicone Travel Infa Feeder, Colors May Vary, 3 Ounce == -1 == 1.15227353848e-18\n",
      "Peg-Perego Tatamia High Chair, White Latte == -1 == 1.26175666136e-18\n",
      "Fisher-Price Royal Potty == -1 == 1.60282966316e-18\n",
      "Safety 1st Exchangeable Tip 3 in 1 Thermometer == -1 == 7.04887411711e-18\n",
      "Safety 1st Lift Lock and Swing Gate == -1 == 9.84839237568e-18\n",
      "Evenflo Take Me Too Premiere Tandem Stroller - Castlebay == -1 == 1.00120730395e-17\n",
      "Cloth Diaper Sprayer--styles may vary == -1 == 1.16906355601e-17\n",
      "The First Years 3 Pack Breastflow Bottle, 9 Ounce == -1 == 1.22003532002e-17\n"
     ]
    }
   ],
   "source": [
    "#print test_data[-1]\n",
    "twenty_rows = test_data.topk('probability',k=20,reverse = True)\n",
    "\n",
    "len(twenty_rows)\n",
    "for i in twenty_rows:\n",
    "    print i['name'],'==',i['sentiment'],'==',i[\"probability\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a \"research-aholic\" in general and have researched many different baby products over the past 5 years due to being a mother of 3 and wanting to get the best for the price and not wanting to settle for less...  I live in the Northeast US, so it gets really cold and the weather changes frequently and in extremes on a daily, if not hourly basis.  Anyway, here are the brands I've used/owed and/or researched extensively and the comparisons:Jolly Jumper Arctic Sneak A Peek Infant Car Seat Cover (JJA)Jolly Jumper Sneak A Peek Infant Car Seat Cover (JJ)Summer Infant Poshpouch (SP)Cozy Car Seat Microfiber and Fleece Cover (CC)Baby's Cozy World Microfiber & Fleece Carrier Cover (BC) (yes the CC and the BC are different many people do not know this....)JJ Cole infant/original bundle me (JJC)JJ Cole toddler bundle me  (JJT)OK...Jolly Jumper Artic (JJA) is awesome, I would rate it the best one out of all.  It is a \"shower cap\" cover for the infant car seat, it has an attached blanket inside and a peek-a-boo flap that you can velcro closed to keep the baby's face covered and warm and you can also velcro it \"up\" to stay open when when you put the baby in the car.  It has two zippers which allow the cover to be fully opened without removal of the cover and easy access to baby.  And lastly, it keeps the baby from over heating.  When opening the two zippers it enable the ability to \"flip\" the middle piece down keeping it completely off the baby.  The blanket is a nice feature of this cover.  You can use it alone with the two zippers open on a warmer day and you can also flip the blanket down away from the baby as well when not in use.  Another nice thing about the blanket is you can wrap it around baby to keep them warm (it does not go behind the baby but can be placed on the sides of the baby), there is enough material on both sides to do this..  Some reviewers of this carrier cover and other \"shower cap\" cover styles stated that there were gaps around the handle, this is true of mostly all the shower cap covers except you can adjust this one on the sides to cover these gaps.  Also you can take the wrap around blanket and cover the baby as well as feed it through the gaps on the sides to close them if the cover cannot be adjusted on your carrier to fit around the bottom to close that area.  The outer portion is nylon, like a jacket material and it is waterproof, the fleece is substantial and it keeps the infant very, warm. This is an expensive carrier but well worth the money!  This is top notch!Jolly Jumper (JJ) is the same carrier cover as above minus the attached blanket.  Again, overall an excellent cover.  It is not much less money than the JJA and no matter what climate I would personally pay the extra money for the JJA just because you can use the blanket when opening the cover by the zippers and then you still have the option of zippering the carrier for more protection if it is raining or it gets colder.  You can also fold down the blanket and not use it as all on the JJA and use it like a JJ.Summer Infant Poshpouch (SP) is another \"shower cap\" cover for the infant carseat but it has an opening in the top with no flap to velcro over baby's face.  I owned this before the JJA and here are some major differences.  It is a much thinner material, the fleece isn't as substantial and the outer portion is not waterproof or nylon but a fleece polyester material.  The opening is rather large and I felt as though breezes/wind would go right through the hole and it is definitely not as effective in making the baby warm, esp. in colder conditions.  To remedy this problem I put a blanket on underside of the cover and the baby warmer, esp on 30 degree days.  I also had to put a hat and make sure the blanket was high on baby to cover the baby's neck or use something to make sure baby's neck was covered due to the hole being so big.  It also gaps much more on the sides then the JJA and the JJ.  The SP only has one zipper to unzip the baby when inside.  And when you want to keep the baby inside the carrier, you really have to take the whole thing off to leave the baby in in order to not be covered.  And it is hard to get the baby out without pushing it aside.AND MOST IMPORTANTLY ******* PLEASE SEE*****  it was very scary when I discovered that when you place it on correctly and put it over the infant carseat, I would put my infant carseat into the holster and it WOULD NOT, I repeat WOULD NOT lATCH INTO THE BASE!!!!  There was no way of knowing this unless you checked, (I check my baby each time to make sure the carseat is latched in, just by habit), but it would not even be partially latched just completely unlatched....  I felt this was unacceptable....  Even when I tried to adjust it by pulling the bottom portion up, where the baby's feet would be, the cover b/c of the elastic would just come up, bunch and be partially off with a gap under the baby's feet so I feel this carrier is a COMPLETE WASTE OF MONEY.  I paid 2/3 the cost of the JJA and it is more then the CC and BC.Cozy Car Seat Microfiber and Fleece Cover (CC) A lot of people think this is the BC and it is the same price but it is not.  It is a cheap spin off of the BC even though it is the same price.  The fabric is less substantial, it is honestly more like the SP.  Not worth the money and don't make the mistake of getting it.Baby's Cozy World Microfiber & Fleece Carrier Cover (BC) uses a type of microfiber called microsuede, which is a soft fabric made from synthetic fibers that are an imitation to suede.  It seems like it is a good bang for your buck coming in around twenty dollars which is half of the JJA.  The only thing is if you live in a really rainy climate I don't think it would be as waterproof being microsuede as the JJ or the JJA material.  Another reviewer stated that the BABY'S COZY WORLD is reversible (which I am not sure how it can be reversible deeming that it does have a flap that goes over the infants face like the CC, JJA and JJ but maybe??)The reviewer also stated that the BC has pacifier clips, which would be a nice feature for toys as well and that is is very thick for all weather and it is the same price as the CC.  ONE THING that many reviews said about the CC and the BC is that the zippers can be dangerous b/c they line up in the same place as a newborns eyes and have the potential for scratching the cornea.  With the JJ and the JJA the zippers do not seem as though they would reach this far.  Maybe the CC and the BC have larger zippers that might spin around in opposite directions esp if it is \"reversible\".  It does have the zippers on both sides (like the JJ and JJA) to unzip and expose the infant nicely when inside.  For the price this seems like it would be a good carrier as well and I would rate the the BC a close second and very comparable to the JJ but does not have a blanket feature like the JJA.JJ Cole infant/original bundle me (JJC)is what I owned with my first child who was a premature infant and I only got about 7 months of use out of this cover due to the fact that infants tend to grow out of it very fast, usually before they are out of their infant carseat.  That is why I got the toddler JJ cole toddler size after.  It is a different type of carrier than all the others that are reviewed above being that it is not a shower cap style it has an underside fabric to the carrier that is placed under the harness straps and then a layer goes over the baby and is zippered down and around the bottom.  FIRST OF ALL THIS IS DANGEROUS, which I never knew as a new mom, and why they even CAN SELL them as carseat covers is beyond me because if you get into an accident it compresses under the infant because of the added bulk between the seat and the child, (it is like the infant is wearing a jacket) so there is more room in between the harness and the infant deeming it unsafe.  Also, JJ cole now has the lite JJ cole and says that it passed crash tests but The Bundle me Lite's openings do not come anywhere near the hip straps of the harness... meaning they would be improperly routed, thus creating slack.  Here is a picture: [...]  No certified carseat technicians recommend this aftermarket product for the carseat.  Any many if not all carseat companies state if an aftermarket product is used \"inside\" the carseat between the harness and baby it will void all guarantees for safety.  NOT GOOD.JJ Cole toddler (JJT)  Had this mostly with my first child (after she was 7 months) and second child and would not use now d/t the same reason above with the JJ.  But it could be nice with a stroller.  The only thing is you do need a hat and something to cover the baby's neck as well.In summary:#1The Jolly Jumper Artic (JJA) wins the race, I am very very please with this product, esp the blanket aspect that I think is well worth it!The JJ is basically the same product as the JJA but without the blanket....#2The Baby's Cozy World Microfiber & Fleece Carrier Cover (BC) (but make sure it is the BC and not the CC when you purchase it) comes in a close second esp for the price!Hope this review is helpful to you and you now won't have to go through the countless hours of research and will be getting a great product while not wasting any of your time or money on another worthless baby item, instead ONLY getting the BEST for your baby!!!!\n"
     ]
    }
   ],
   "source": [
    "print twenty_rows['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question**: Which of the following products are represented in the 20 most negative reviews?  [multiple choice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy of the classifier\n",
    "\n",
    "We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified examples}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "This can be computed as follows:\n",
    "\n",
    "* **Step 1:** Use the trained model to compute class predictions (**Hint:** Use the `predict` method)\n",
    "* **Step 2:** Count the number of data points when the predicted class labels match the ground truth labels (called `true_labels` below).\n",
    "* **Step 3:** Divide the total number of correct predictions by the total number of data points in the dataset.\n",
    "\n",
    "Complete the function below to compute the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    # First get the predictions\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    model_predictions = model.predict(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute the number of correctly classified examples\n",
    "    ## YOUR CODE HERE\n",
    "    num_correct = 0\n",
    "    for i in range(len(data)):\n",
    "        if true_labels[i] == model_predictions[i]:\n",
    "            num_correct = num_correct + 1\n",
    "    \n",
    "    \n",
    "\n",
    "    # Then compute accuracy by dividing num_correct by total number of examples\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    accuracy = num_correct / len(data)\n",
    "    \n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+--------+\n",
      "|              name             |             review            | rating |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "| Baby Tracker&reg; - Daily ... | This has been an easy way ... |  4.0   |\n",
      "| Baby Tracker&reg; - Daily ... | I love this journal and ou... |  4.0   |\n",
      "| Nature's Lullabies First Y... | I love this little calende... |  5.0   |\n",
      "| Nature's Lullabies Second ... | I had a hard time finding ... |  5.0   |\n",
      "|  Lamaze Peekaboo, I Love You  | One of baby's first and fa... |  4.0   |\n",
      "|  Lamaze Peekaboo, I Love You  | My son loved this book as ... |  5.0   |\n",
      "|  Lamaze Peekaboo, I Love You  | Our baby loves this book &... |  5.0   |\n",
      "| SoftPlay Giggle Jiggle Fun... | This bear is absolutely ad... |  2.0   |\n",
      "| SoftPlay Peek-A-Boo Where'... | I bought two for recent ba... |  5.0   |\n",
      "| Baby's First Year Undated ... | I searched high and low fo... |  5.0   |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "+-------------------------------+-----------+----------------+------------------+\n",
      "|           word_count          | sentiment |  probability   | model_prediction |\n",
      "+-------------------------------+-----------+----------------+------------------+\n",
      "| {'all': 1L, 'standarad': 1... |     1     | 0.758399887752 |        1         |\n",
      "| {'all': 2L, 'nannys': 1L, ... |     1     | 0.999999999966 |        1         |\n",
      "| {'and': 1L, 'babys': 1L, '... |     1     | 0.22895097808  |        -1        |\n",
      "| {'and': 3L, 'all': 1L, 'mo... |     1     | 0.999999558063 |        1         |\n",
      "| {'and': 2L, 'because': 1L,... |     1     | 0.990542169248 |        1         |\n",
      "| {'all': 1L, 'being': 1L, '... |     1     | 0.999999295968 |        1         |\n",
      "| {'and': 1L, 'own': 1L, 'it... |     1     | 0.99976447628  |        1         |\n",
      "| {'and': 3L, 'cute': 1L, 'r... |     -1    | 0.722834466283 |        1         |\n",
      "| {'beautiful': 1L, 'and': 2... |     1     | 0.999266840896 |        1         |\n",
      "| {'remembering': 1L, 'and':... |     1     | 0.999786830048 |        1         |\n",
      "+-------------------------------+-----------+----------------+------------------+\n",
      "[33336 rows x 7 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "test_data['model_prediction'] = sentiment_model.predict(test_data)\n",
    "print test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the classification accuracy of the **sentiment_model** on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9145368370530358"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(sentiment_model, test_data, test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9145368370530358,\n",
       " 'auc': 0.9250003098558346,\n",
       " 'confusion_matrix': Columns:\n",
       " \ttarget_label\tint\n",
       " \tpredicted_label\tint\n",
       " \tcount\tint\n",
       " \n",
       " Rows: 4\n",
       " \n",
       " Data:\n",
       " +--------------+-----------------+-------+\n",
       " | target_label | predicted_label | count |\n",
       " +--------------+-----------------+-------+\n",
       " |      -1      |        -1       |  3798 |\n",
       " |      -1      |        1        |  1443 |\n",
       " |      1       |        -1       |  1406 |\n",
       " |      1       |        1        | 26689 |\n",
       " +--------------+-----------------+-------+\n",
       " [4 rows x 3 columns],\n",
       " 'f1_score': 0.9493303928717519,\n",
       " 'log_loss': 0.3707618266205636,\n",
       " 'precision': 0.9487060998151571,\n",
       " 'recall': 0.9499555080975263,\n",
       " 'roc_curve': Columns:\n",
       " \tthreshold\tfloat\n",
       " \tfpr\tfloat\n",
       " \ttpr\tfloat\n",
       " \tp\tint\n",
       " \tn\tint\n",
       " \n",
       " Rows: 100001\n",
       " \n",
       " Data:\n",
       " +-----------+----------------+----------------+-------+------+\n",
       " | threshold |      fpr       |      tpr       |   p   |  n   |\n",
       " +-----------+----------------+----------------+-------+------+\n",
       " |    0.0    |      1.0       |      1.0       | 28095 | 5241 |\n",
       " |   1e-05   | 0.849647013929 | 0.996689802456 | 28095 | 5241 |\n",
       " |   2e-05   | 0.829994275902 | 0.996262680192 | 28095 | 5241 |\n",
       " |   3e-05   | 0.820072505247 | 0.996013525538 | 28095 | 5241 |\n",
       " |   4e-05   | 0.811486357565 | 0.995799964406 | 28095 | 5241 |\n",
       " |   5e-05   | 0.805189849265 | 0.995657590319 | 28095 | 5241 |\n",
       " |   6e-05   | 0.799656554093 | 0.995515216231 | 28095 | 5241 |\n",
       " |   7e-05   | 0.794314062202 | 0.995372842143 | 28095 | 5241 |\n",
       " |   8e-05   | 0.789543980156 | 0.995230468055 | 28095 | 5241 |\n",
       " |   9e-05   | 0.786681930929 | 0.995123687489 | 28095 | 5241 |\n",
       " +-----------+----------------+----------------+-------+------+\n",
       " [100001 rows x 5 columns]\n",
       " Note: Only the head of the SFrame is printed.\n",
       " You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the accuracy of the **sentiment_model** on the **test_data**? Round your answer to 2 decimal places (e.g. 0.76).\n",
    "\n",
    "**Question**: Does a higher accuracy value on the **training_data** always imply that the classifier is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn another classifier with fewer words\n",
    "\n",
    "There were a lot of words in the model we trained above. We will now train a simpler logistic regression model using only a subset of words that occur in the reviews. For this assignment, we selected a 20 words to work with. These are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(significant_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each review, we will use the **word_count** column and trim out all words that are **not** in the **significant_words** list above. We will use the [SArray dictionary trim by keys functionality]( https://dato.com/products/create/docs/generated/graphlab.SArray.dict_trim_by_keys.html). Note that we are performing this on both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['word_count_subset'] = train_data['word_count'].dict_trim_by_keys(significant_words, exclude=False)\n",
    "test_data['word_count_subset'] = test_data['word_count'].dict_trim_by_keys(significant_words, exclude=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the first example of the dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it came early and was not disappointed. i love planet wise bags and now my wipe holder. it keps my osocozy wipes moist and does not leak. highly recommend it.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **word_count** column had been working with before looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 3L, 'love': 1L, 'it': 3L, 'highly': 1L, 'osocozy': 1L, 'bags': 1L, 'leak': 1L, 'moist': 1L, 'does': 1L, 'recommend': 1L, 'was': 1L, 'wipes': 1L, 'disappointed': 1L, 'early': 1L, 'not': 2L, 'now': 1L, 'holder': 1L, 'wipe': 1L, 'keps': 1L, 'wise': 1L, 'i': 1L, 'planet': 1L, 'my': 2L, 'came': 1L}\n"
     ]
    }
   ],
   "source": [
    "print train_data[0]['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only working with a subset of these words, the column **word_count_subset** is a subset of the above dictionary. In this example, only 2 `significant words` are present in this review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1L, 'disappointed': 1L}\n"
     ]
    }
   ],
   "source": [
    "print train_data[0]['word_count_subset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a logistic regression model on a subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a classifier with **word_count_subset** as the feature and **sentiment** as the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Logistic regression:</pre>"
      ],
      "text/plain": [
       "Logistic regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 133416</pre>"
      ],
      "text/plain": [
       "Number of examples          : 133416"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 1</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 20</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 21</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-accuracy |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-accuracy |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 0.290366     | 0.862917          |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 0.290366     | 0.862917          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 3        | 0.426287     | 0.865713          |</pre>"
      ],
      "text/plain": [
       "| 2         | 3        | 0.426287     | 0.865713          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 4        | 0.552216     | 0.866478          |</pre>"
      ],
      "text/plain": [
       "| 3         | 4        | 0.552216     | 0.866478          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 5        | 0.688713     | 0.866748          |</pre>"
      ],
      "text/plain": [
       "| 4         | 5        | 0.688713     | 0.866748          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 5         | 6        | 0.847620     | 0.866815          |</pre>"
      ],
      "text/plain": [
       "| 5         | 6        | 0.847620     | 0.866815          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 6         | 7        | 0.978545     | 0.866815          |</pre>"
      ],
      "text/plain": [
       "| 6         | 7        | 0.978545     | 0.866815          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Class                          : LogisticClassifier\n",
       "\n",
       "Schema\n",
       "------\n",
       "Number of coefficients         : 21\n",
       "Number of examples             : 133416\n",
       "Number of classes              : 2\n",
       "Number of feature columns      : 1\n",
       "Number of unpacked features    : 20\n",
       "\n",
       "Hyperparameters\n",
       "---------------\n",
       "L1 penalty                     : 0.0\n",
       "L2 penalty                     : 0.01\n",
       "\n",
       "Training Summary\n",
       "----------------\n",
       "Solver                         : newton\n",
       "Solver iterations              : 6\n",
       "Solver status                  : SUCCESS: Optimal solution found.\n",
       "Training time (sec)            : 1.012\n",
       "\n",
       "Settings\n",
       "--------\n",
       "Log-likelihood                 : 44323.7254\n",
       "\n",
       "Highest Positive Coefficients\n",
       "-----------------------------\n",
       "word_count_subset[loves]       : 1.6773\n",
       "word_count_subset[perfect]     : 1.5145\n",
       "word_count_subset[love]        : 1.3654\n",
       "(intercept)                    : 1.2995\n",
       "word_count_subset[easy]        : 1.1937\n",
       "\n",
       "Lowest Negative Coefficients\n",
       "----------------------------\n",
       "word_count_subset[disappointed] : -2.3551\n",
       "word_count_subset[return]      : -2.1173\n",
       "word_count_subset[waste]       : -2.0428\n",
       "word_count_subset[broke]       : -1.658\n",
       "word_count_subset[money]       : -0.8979"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model = graphlab.logistic_classifier.create(train_data,\n",
    "                                                   target = 'sentiment',\n",
    "                                                   features=['word_count_subset'],\n",
    "                                                   validation_set=None)\n",
    "simple_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the classification accuracy using the `get_classification_accuracy` function you implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8693004559635229,\n",
       " 'auc': 0.828809641857926,\n",
       " 'confusion_matrix': Columns:\n",
       " \ttarget_label\tint\n",
       " \tpredicted_label\tint\n",
       " \tcount\tint\n",
       " \n",
       " Rows: 4\n",
       " \n",
       " Data:\n",
       " +--------------+-----------------+-------+\n",
       " | target_label | predicted_label | count |\n",
       " +--------------+-----------------+-------+\n",
       " |      1       |        -1       |  469  |\n",
       " |      -1      |        -1       |  1353 |\n",
       " |      -1      |        1        |  3888 |\n",
       " |      1       |        1        | 27626 |\n",
       " +--------------+-----------------+-------+\n",
       " [4 rows x 3 columns],\n",
       " 'f1_score': 0.9269070106863058,\n",
       " 'log_loss': 0.3296871005223945,\n",
       " 'precision': 0.8766262613441645,\n",
       " 'recall': 0.983306638191849,\n",
       " 'roc_curve': Columns:\n",
       " \tthreshold\tfloat\n",
       " \tfpr\tfloat\n",
       " \ttpr\tfloat\n",
       " \tp\tint\n",
       " \tn\tint\n",
       " \n",
       " Rows: 100001\n",
       " \n",
       " Data:\n",
       " +-----------+----------------+----------------+-------+------+\n",
       " | threshold |      fpr       |      tpr       |   p   |  n   |\n",
       " +-----------+----------------+----------------+-------+------+\n",
       " |    0.0    |      1.0       |      1.0       | 28095 | 5241 |\n",
       " |   1e-05   | 0.999809196718 | 0.999964406478 | 28095 | 5241 |\n",
       " |   2e-05   | 0.999809196718 | 0.999964406478 | 28095 | 5241 |\n",
       " |   3e-05   | 0.999809196718 | 0.999928812956 | 28095 | 5241 |\n",
       " |   4e-05   | 0.999809196718 | 0.999928812956 | 28095 | 5241 |\n",
       " |   5e-05   | 0.999809196718 | 0.999928812956 | 28095 | 5241 |\n",
       " |   6e-05   | 0.999809196718 | 0.999928812956 | 28095 | 5241 |\n",
       " |   7e-05   | 0.999809196718 | 0.999928812956 | 28095 | 5241 |\n",
       " |   8e-05   | 0.999809196718 | 0.999928812956 | 28095 | 5241 |\n",
       " |   9e-05   | 0.999809196718 | 0.999928812956 | 28095 | 5241 |\n",
       " +-----------+----------------+----------------+-------+------+\n",
       " [100001 rows x 5 columns]\n",
       " Note: Only the head of the SFrame is printed.\n",
       " You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8693004559635229"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(simple_model, test_data, test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will inspect the weights (coefficients) of the **simple_model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-------+-----------------+-----------------+\n",
      "|        name       |    index     | class |      value      |      stderr     |\n",
      "+-------------------+--------------+-------+-----------------+-----------------+\n",
      "|    (intercept)    |     None     |   1   |   1.2995449552  | 0.0120888541331 |\n",
      "| word_count_subset | disappointed |   1   |  -2.35509250061 | 0.0504149888557 |\n",
      "| word_count_subset |     love     |   1   |  1.36543549368  | 0.0303546295109 |\n",
      "| word_count_subset |     well     |   1   |  0.504256746398 |  0.021381300631 |\n",
      "| word_count_subset |   product    |   1   | -0.320555492996 | 0.0154311321362 |\n",
      "| word_count_subset |    loves     |   1   |  1.67727145556  | 0.0482328275384 |\n",
      "| word_count_subset |    little    |   1   |  0.520628636025 | 0.0214691475665 |\n",
      "| word_count_subset |     work     |   1   | -0.621700012425 | 0.0230330597946 |\n",
      "| word_count_subset |     easy     |   1   |  1.19366189833  |  0.029288869202 |\n",
      "| word_count_subset |    great     |   1   |  0.94469126948  | 0.0209509926591 |\n",
      "+-------------------+--------------+-------+-----------------+-----------------+\n",
      "[21 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "print simple_model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the coefficients (in descending order) by the **value** to obtain the coefficients with the most positive effect on the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_model_weights = simple_model.coefficients.sort('value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Consider the coefficients of **simple_model**. There should be 21 of them, an intercept term + one for each word in **significant_words**. How many of the 20 coefficients (corresponding to the 20 **significant_words** and *excluding the intercept term*) are positive for the `simple_model`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "simple_model_positive_words = []\n",
    "\n",
    "for i in range(len(simple_model_weights)):\n",
    "    if simple_model_weights['value'][i] >= 0 and simple_model_weights['name'][i] != '(intercept)':\n",
    "        #print simple_model_weights['value'][i], simple_model_weights['name'][i], simple_model_weights['class'][i], simple_model_weights['index'][i]\n",
    "        simple_model_positive_words.append(simple_model_weights['index'][i])\n",
    "        count = count + 1\n",
    "        \n",
    "print count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Are the positive words in the **simple_model** (let us call them `positive_significant_words`) also positive words in the **sentiment_model**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------+\n",
      "| positive_negative_weights | count |\n",
      "+---------------------------+-------+\n",
      "|          positive         | 68419 |\n",
      "|          negative         | 53294 |\n",
      "+---------------------------+-------+\n",
      "[2 rows x 2 columns]\n",
      "\n",
      "lenght of sentiment_model_weights:  121713\n",
      "lenght of positive_name:  68419\n",
      "lenght of negative_name:  53294\n",
      "lenght of positive_name plus lenght of negative_name:  121713\n"
     ]
    }
   ],
   "source": [
    "sentiment_model_weights = sentiment_model.coefficients.sort('value', ascending=False)\n",
    "sentiment_model_weights['positive_negative_weights'] = sentiment_model_weights['value'].apply(lambda x: 'positive' if x>=0 else 'negative')\n",
    "print sentiment_model_weights.groupby(key_columns='positive_negative_weights',\n",
    "                         operations={'count': graphlab.aggregate.COUNT()})\n",
    "positive_name = sentiment_model_weights[\"index\"][sentiment_model_weights['positive_negative_weights'] == 'positive']\n",
    "negative_name = sentiment_model_weights['index'][sentiment_model_weights['positive_negative_weights'] == 'negative']\n",
    "print 'lenght of sentiment_model_weights: ',len(sentiment_model_weights)\n",
    "print 'lenght of positive_name: ',len(positive_name)\n",
    "print 'lenght of negative_name: ',len(negative_name)\n",
    "print 'lenght of positive_name plus lenght of negative_name: ',len(positive_name) + len(negative_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the accuracy of the **sentiment_model** and the **simple_model** using the `get_classification_accuracy` method you implemented above.\n",
    "\n",
    "First, compute the classification accuracy of the **sentiment_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979440247046831"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(sentiment_model,train_data,train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the classification accuracy of the **simple_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8668150746537147"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(simple_model,train_data,train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TRAINING set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will repeat this exercise on the **test_data**. Start by computing the classification accuracy of the **sentiment_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9145368370530358"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(sentiment_model,test_data,test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the classification accuracy of the **simple_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8693004559635229"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(simple_model,test_data,test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TEST set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "It is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "What is the majority class in the **train_data**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112164\n",
      "21252\n"
     ]
    }
   ],
   "source": [
    "num_positive  = (train_data['sentiment'] == +1).sum()\n",
    "num_negative = (train_data['sentiment'] == -1).sum()\n",
    "print num_positive\n",
    "print num_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the accuracy of the majority class classifier on **test_data**.\n",
    "\n",
    "**Question**: Enter the accuracy of the majority class classifier model on the **test_data**. Round your answer to two decimal places (e.g. 0.76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842782577394\n"
     ]
    }
   ],
   "source": [
    "print (test_data['sentiment'] == +1).sum()/len(test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is the **sentiment_model** definitely better than the majority class classifier (the baseline)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
